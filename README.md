# Automatic Summarization
Final Master thesis 

- we first **scrape** politifact data from [**politifact.com**](https://www.politifact.com/), script saved in [**here**](https://github.com/petervajdecka02947/MasterThesis2022/blob/main/P0.politifact_scraping.ipynb) with its functions [**here**](https://github.com/petervajdecka02947/MasterThesis2022/blob/main/utils/scraping.py)
- **preprocessing of data** is saved in [**here**](https://github.com/petervajdecka02947/MasterThesis2022/blob/main/P1.data_preprocess.ipynb) with its functions [**here**](https://github.com/petervajdecka02947/MasterThesis2022/blob/main/utils/preprocess.py)
- data are splitted in ration **train/dev/test = 80/10/10**
- **Bert was fine-tunned** in three different ways, the script of the best results are saved in [**here**](https://github.com/petervajdecka02947/MasterThesis2022/blob/main/P2B.Bert_fine-tunning.ipynb) with its functions [**here**](https://github.com/petervajdecka02947/MasterThesis2022/blob/main/utils/bert.py)
- splitted sentences of rulling comments:
  - **tf-idf and doc2vec** vectorizations were applied,  script is directed [**here**](https://github.com/petervajdecka02947/MasterThesis2022/blob/main/P3B.tf-idf_doc2vec.ipynb) with its functions [**here**](https://github.com/petervajdecka02947/MasterThesis2022/blob/main/utils/tf_idf_doc2vec.py)
  - **Bert vectorization** vectorizations was applied, script is directed [**here**](https://github.com/petervajdecka02947/MasterThesis2022/blob/main/P3A.source_text_similarity.ipynb) with its functions [**here**](https://github.com/petervajdecka02947/MasterThesis2022/blob/main/utils/split_embeds.py)
- **Local Outlier factor(LOF)** was calculated by given vector representation for each sentence of whole corpus and those LOF values, certain amount of **sentences** were **removed**, script is saved [**here**](https://github.com/petervajdecka02947/MasterThesis2022/blob/main/P4.selection_on_similarity.ipynb) with its functions [**here**](https://github.com/petervajdecka02947/MasterThesis2022/blob/main/utils/selection.py)
- After data are substracted, we are ready to **fine-tune unified text to tex transformer (T5)**, script in [**here**](https://github.com/petervajdecka02947/MasterThesis2022/blob/main/P5.t5_fine-tunning.ipynb) with its functions [**here**](https://github.com/petervajdecka02947/MasterThesis2022/blob/main/utils/t5.py)
- Now we have fine-tunned model by train and dev data and this model is saved, then we call this model and generate summaries on test data only, script in [**here**](https://github.com/petervajdecka02947/MasterThesis2022/blob/main/P6.Few%20shot%20generation.ipynb) with its functions [**here**](https://github.com/petervajdecka02947/MasterThesis2022/blob/main/utils/t5.py)
- Performance is measured using **Rouge Score** again only on test data, where gold summaries are compared to **generated summaries**, script saved in [**here**](https://github.com/petervajdecka02947/MasterThesis2022/blob/main/P7.t5_peformance_testing.ipynb)
